import math
import re
import sys
from collections import Counter
import requests  # ç”¨äºè°ƒç”¨é˜¿é‡Œäº‘ç™¾ç‚¼ API

# ç¡®ä¿å®‰è£…äº†æ‰€éœ€çš„åº“
try:
    import jieba
    import pandas as pd
    import streamlit as st
except ImportError:
    # å¼•å¯¼ç”¨æˆ·å®‰è£…ä¾èµ–
    print("é”™è¯¯ï¼šè¯·å…ˆå®‰è£…æ‰€éœ€çš„åº“ã€‚è¿è¡Œå‘½ä»¤ï¼špip3 install jieba pandas streamlit")
    sys.exit(1)


# ===== 1. é…ç½®ï¼šè¦è¿‡æ»¤æ‰çš„ç±»ç›® & å™ªéŸ³è¯ =====

STOP_CATEGORIES = {
    "å½±è§†å¨±ä¹", "ä½“è‚²èµ›äº‹", "å®¶å±…å®¶è£…", "æ—…æ¸¸å‡ºè¡Œ", "ç¾å¦†æ—¶å°š",
    "ç¤¾ä¼šçƒ­ç‚¹", "æ¸¸æˆç«æŠ€", "å…¶ä»–", "çƒ­é—¨æ¦œå•",
}

PLATFORM_NUM_PATTERNS = [
    re.compile(r"å°çº¢ä¹¦\s*[\d,\.]+"),
    re.compile(r"æŠ–éŸ³\s*[\d,\.]+"),
    re.compile(r"å¿«æ‰‹\s*[\d,\.]+"),
]

GENERIC_KEYS = [
    "çƒ­æ¢—", "çƒ­ç‚¹", "æµè¡Œ", "è¶‹åŠ¿", "è¯é¢˜", "æ–‡åŒ–", "è¥é”€", "ç­–ç•¥", "è¿è¥", "å†…å®¹",
    "å“ç‰Œ", "ç”¨æˆ·", "ç¤¾äº¤", "ç¤¾åª’", "æ–°åª’ä½“", "æ•´åˆè¥é”€", "ä¼ æ’­", "çŸ­è§†é¢‘",
    "ç³»åˆ—", "æˆ‘ä»¬", "ä»–ä»¬", "å¤§å®¶", "å¾ˆå¤šäºº", "å¹´è½»äºº"
]

GENERIC_EXACT = set([
    "çƒ­æ¢—æµè¡Œ", "ç½‘ç»œçƒ­æ¢—", "æµè¡Œè¯", "çƒ­é—¨è¶‹åŠ¿", "çƒ­ç‚¹è¶‹åŠ¿", "ç¤¾äº¤è¶‹åŠ¿",
    "æ•´åˆè¥é”€ç­–ç•¥", "ç¤¾åª’æ•´åˆè¥é”€", "ç½‘ç»œçƒ­ç‚¹",
])

# ===== æ–°å¢ï¼šäººå & å›½å®¶å è¿‡æ»¤é…ç½® =====

# å¸¸è§ä¸­æ–‡å§“æ°ï¼ˆè¦†ç›–ç»å¤§éƒ¨åˆ† 2~3 å­—äººåï¼‰
COMMON_SURNAMES = set(list(
    "èµµé’±å­™æå‘¨å´éƒ‘ç‹å†¯é™ˆè¤šå«è’‹æ²ˆéŸ©æ¨æœ±ç§¦å°¤è®¸ä½•å•æ–½å¼ "
    "å­”æ›¹ä¸¥åé‡‘é­é™¶å§œæˆšè°¢é‚¹å–»æŸæ°´çª¦ç« äº‘è‹æ½˜è‘›å¥šèŒƒå½­éƒ"
    "é²éŸ¦æ˜Œé©¬è‹—å‡¤èŠ±æ–¹ä¿ä»»è¢æŸ³é…†é²å²å”è´¹å»‰å²‘è–›é›·è´ºå€ªæ±¤"
    "æ»•æ®·ç½—æ¯•éƒé‚¬å®‰å¸¸ä¹äºæ—¶å‚…çš®åé½åº·ä¼ä½™å…ƒåœé¡¾å­Ÿå¹³é»„"
    "å’Œç©†è§å°¹å§šé‚µæ¹›æ±ªç¥æ¯›ç¦¹ç‹„ç±³è´æ˜è‡§è®¡ä¼æˆæˆ´è°ˆå®‹èŒ…åº"
    "ç†Šçºªèˆ’å±ˆé¡¹ç¥è‘£æ¢æœé˜®è“é—µå¸­å­£éº»å¼ºè´¾è·¯å¨„å±æ±Ÿç«¥é¢œéƒ­"
    "æ¢…ç››æ—åˆé’Ÿå¾é‚±éª†é«˜å¤è”¡ç”°æ¨Šèƒ¡å‡Œéœè™ä¸‡æ”¯æŸ¯æ˜ç®¡å¢è«"
    "æˆ¿è£˜ç¼ªå¹²è§£åº”å®—ä¸å®£è´²é‚“éƒå•æ­æ´ªåŒ…è¯¸å·¦çŸ³å´”å‰é’®é¾šç¨‹"
    "åµ‡é‚¢æ»‘è£´é™†è£ç¿è€ç¾Šæ–¼æƒ ç”„å¥šæ¡‘æ¡‚æ¿®ç‰›å¯¿é€šè¾¹æ‰ˆç‡•å†€éƒ"
    "æµ¦å°šå†œæ¸©åˆ«åº„æ™æŸ´ç¿é˜å……æ…•è¿èŒ¹ä¹ å®¦è‰¾é±¼å®¹å‘å¤æ˜“æ…æˆˆ"
    "å»–åºšç»ˆæš¨å±…è¡¡æ­¥éƒ½è€¿æ»¡å¼˜åŒ¡å›½æ–‡å¯‡å¹¿ç¦„é˜™ä¸œæ®´æ®³æ²ƒåˆ©è”š"
    "è¶Šå¤”éš†å¸ˆå·©åè‚æ™å‹¾æ•–èå†·è¨¾è¾›é˜šé‚£ç®€é¥¶ç©ºæ›¾æ¯‹æ²™ä¹œå…»"
    "é é¡»ä¸°å·¢å…³è’¯ç›¸æŸ¥åè†çº¢æ¸¸ç«ºæƒé€¯ç›–ç›Šæ¡“å…¬ä¸‡ä¿Ÿå¸é©¬ä¸Š"
    "æ¬§é˜³å¤ä¾¯è¯¸è‘›é—»äººä¸œæ–¹èµ«è¿çš‡ç”«å°‰è¿Ÿå…¬ç¾Šæ¾¹å°å…¬å†¶å®—æ”¿"
    "æ¿®é˜³æ·³äºå•äºå¤ªå”ç”³å± å…¬å­™ä»²å­™è½©è¾•ä»¤ç‹é’Ÿç¦»å®‡æ–‡é•¿å­™"
    "æ…•å®¹é²œäºé—¾ä¸˜å¸å¾’å¸ç©º"
))

# å¸¸è§å›½å®¶ / åœ°ç¼˜è¯ï¼ˆä½ å¯ä»¥ä»¥åè‡ªå·±å¾€é‡ŒåŠ ï¼‰
COUNTRY_WORDS = [
    "ä¸­å›½", "å¤§é™†", "å†…åœ°", "æ¸¯æ¾³å°",
    "ç¾å›½", "è‹±", "è‹±å›½", "æ³•å›½", "å¾·", "å¾·å›½",
    "æ—¥æœ¬", "æ—¥éŸ©", "éŸ©å›½", "æœé²œ",
    "ä¿„ç½—æ–¯", "ä¿„å›½", "è‹è”", "ä¹Œå…‹å…°",
    "å°åº¦", "è¶Šå—", "æ³°å›½", "æ–°åŠ å¡", "é©¬æ¥è¥¿äºš", "å°å°¼", "è²å¾‹å®¾",
    "æ¾³å¤§åˆ©äºš", "åŠ æ‹¿å¤§", "å¢¨è¥¿å“¥", "å·´è¥¿", "é˜¿æ ¹å»·",
    "ä¸­ä¸œ", "ä»¥è‰²åˆ—", "å·´å‹’æ–¯å¦", "åŠ æ²™",
    "éæ´²", "æ¬§æ´²", "æ‹‰ç¾", "äºšå¤ª"
]


def looks_like_noise(term: str) -> bool:
    """åˆ¤æ–­ä¸€ä¸ªå€™é€‰çŸ­è¯­æ˜¯ä¸æ˜¯å™ªéŸ³"""
    t = term.strip()
    if not t:
        return True

    t_compressed = re.sub(r"\s+", "", t)

    if t_compressed in GENERIC_EXACT:
        return True
    if t_compressed in STOP_CATEGORIES:
        return True

    # åŒ…å«å…¸å‹æ³›æ¦‚å¿µè¯ä¸”é•¿åº¦è¾ƒçŸ­çš„ï¼Œè§†ä¸ºå™ªéŸ³
    for key in GENERIC_KEYS:
        if key in t_compressed and len(t_compressed) < len(key) + 3:
            return True

    if re.fullmatch(r"[0-9,\.]+", t_compressed):
        return True

    if len(t_compressed) <= 2 and not re.search(r"[0-9A-Za-z]", t_compressed):
        return True

    return False


# ===== æ–°å¢ï¼šäººå / å›½å®¶è¯ è¿‡æ»¤å‡½æ•° =====

def looks_like_person_name(text: str) -> bool:
    """
    ç²—ç•¥åˆ¤æ–­æ˜¯å¦åƒä¸­æ–‡äººåï¼š
    - å»æ‰ç©ºç™½åé•¿åº¦ä¸º 2 æˆ– 3
    - å…¨æ˜¯ä¸­æ–‡
    - ç¬¬ä¸€ä¸ªå­—æ˜¯å¸¸è§å§“æ°
    ï¼ˆä¼šæœ‰å°‘é‡è¯¯æ€ï¼Œä½†å¯¹ä½ â€œä¸è¦äººåæ¢—â€çš„ç›®æ ‡æ˜¯ ok çš„ï¼‰
    """
    if not text:
        return False
    t = re.sub(r"\s+", "", text)
    # åªè€ƒè™‘è¾ƒçŸ­çš„ 2~3 å­—ä¸²
    if len(t) not in (2, 3):
        return False
    # å¿…é¡»å…¨æ˜¯ä¸­æ–‡
    if not all('\u4e00' <= ch <= '\u9fff' for ch in t):
        return False
    # ç¬¬ä¸€ä¸ªå­—æ˜¯å¸¸è§å§“æ°
    return t[0] in COMMON_SURNAMES


def contains_country_or_name(text: str) -> bool:
    """æ˜¯å¦åŒ…å«å›½å®¶åæˆ–çœ‹èµ·æ¥åƒäººå"""
    if not text:
        return False
    t = re.sub(r"\s+", "", text)

    # å›½å®¶ / åœ°ç¼˜å…³é”®è¯
    for w in COUNTRY_WORDS:
        if w and w in t:
            return True

    # åƒäººå
    if looks_like_person_name(t):
        return True

    return False


# ===== 1.5 è§„åˆ™åˆ†ç±»ï¼ˆå¤©æ°” / å®¶äºº / æ‰“å·¥äºº / å® ç‰© / ç”Ÿæ´»æ–¹å¼ï¼‰ =====

CATEGORY_RULES = [
    {
        "name": "å­£èŠ‚/å¤©æ°”/æ¸©åº¦æ¢—",
        "short": "å¤©æ°”æ¸©åº¦",
        "keywords": [
            "å†·ç©ºæ°”", "é™æ¸©", "å…¥å†¬", "ä¸€å¤œæ¢å­£", "æ¢å­£", "æš–å†¬", "å›æš–",
            "æ—©æ™šå†·", "ä¸­åˆçƒ­", "ä¸‹é›ª", "é›¨å¤¹é›ª", "å›å—å¤©", "æ¹¿å†·", "å¹²å†·",
            "é£å¤§", "æš´é›¨", "é«˜æ¸©", "çƒ­æµª", "ç©ºè°ƒ", "æš–æ°”", "åœ°æš–",
            "æ¸©åº¦", "ä½“æ„Ÿ", "ç¾½ç»’æœ", "ç§‹è£¤", "æ£‰è¢„", "çŸ­è¢–", "ç©¿è¡£"
        ]
    },
    {
        "name": "å®¶é‡Œäºº/å®¶åŠ¡æ—¥å¸¸æ¢—",
        "short": "å®¶äººå®¶åŠ¡",
        "keywords": [
            "å­©å­", "å®å®", "å¨ƒ", "ç†Šå­©å­", "å†™ä½œä¸š", "å¯’å‡", "æ”¾å‡åœ¨å®¶",
            "å¦ˆå¦ˆ", "å¦ˆ", "è€å¦ˆ", "çˆ¸çˆ¸", "çˆ¸", "è€çˆ¹", "çˆ¶æ¯", "å…¬å©†",
            "åœ¨å®¶", "å®…å®¶", "å›å®¶", "ä¸‹ç­å›å®¶", "å®¶é‡Œ", "å…¨å®¶",
            "æ´—æ¾¡", "æ´—å¤´", "æ´—è¡£æœ", "æ™¾è¡£æœ", "å®¶åŠ¡", "åšé¥­",
            "å«Œå†·", "å«Œçƒ­", "å«Œæ½®", "å«Œå‘³"
        ]
    },
    {
        "name": "æ‰“å·¥äºº/åŸå¸‚ç”Ÿæ´»æ¢—",
        "short": "æ‰“å·¥äºº",
        "keywords": [
            "æ‰“å·¥äºº", "ä¸Šç­", "ä¸‹ç­", "é€šå‹¤", "æ—©å…«", "æ™šå…«", "åŠ ç­",
            "å·¥ä½", "åŠå…¬å®¤", "å·¥ç‰Œ", "æ‰“å¡", "ç‰›é©¬", "ç¤¾ç•œ",
            "æœˆè–ª", "å·¥èµ„", "ç¤¾ä¿", "å…¬ç§¯é‡‘", "åœ°é“", "åŸå·´", "åŸå·´ä½¬",
            "æˆ‘çš„å·¥ä½œæµç¨‹", "æµç¨‹belike", "æµç¨‹ belike"
        ]
    },
    {
        "name": "å® ç‰©æ¢—ï¼ˆçŒ«çŒ«ç‹—ç‹—ï¼‰",
        "short": "å® ç‰©",
        "keywords": [
            "çŒ«", "çŒ«çŒ«", "ç‹—", "ç‹—ç‹—", "å°ç‹—", "å°çŒ«", "ä¸»å­",
            "é“²å±å®˜", "å® ç‰©", "çŒ«æ¯›", "ç‹—æ¯›", "æ‰æ¯›", "çŒ«çª", "çŒ«ç ‚",
            "æ±ª", "å–µ"
        ]
    },
    {
        "name": "ç”Ÿæ´»æ–¹å¼/å®¡ç¾è¶‹åŠ¿æ¢—",
        "short": "ç”Ÿæ´»æ–¹å¼",
        "keywords": [
            "å›½é£", "å›½æ½®", "æ–°ä¸­å¼", "å”é£", "æ–°ä¸­å¼å®¶", "å®¶è£…",
            "æ”¹é€ ", "è£…ä¿®", "å˜åºŸä¸ºå®", "æ—§ç‰©æ”¹é€ ",
            "è¿åŠ¨æ‰“å¡", "æ‰“å¡", "100å¤©", "ä¸€ç™¾å¤©", "æŒ‘æˆ˜",
            "å¥èº«", "è·‘æ­¥", "éª‘è¡Œ", "å¥åº·", "å‡è„‚", "å…»ç”Ÿ",
            "æç®€", "æ–­èˆç¦»", "ç¾æ‹‰å¾·", "å¤šå·´èƒºç©¿æ­"
        ]
    }
]


def classify_example_text(text: str) -> str:
    """
    è§„åˆ™ç²—åˆ†ï¼šæ ¹æ®ç¤ºä¾‹æ–‡æ¡ˆæ‰“æ ‡ç­¾ã€‚
    - å‘½ä¸­å¤šä¸ªæ—¶ï¼Œç”¨ 'ã€' æ‹¼æ¥
    - éƒ½æ²¡å‘½ä¸­åˆ™æ ‡è®°ä¸º 'æœªåˆ†ç±»/å…¶ä»–'
    """
    if not text:
        return "æœªåˆ†ç±»/å…¶ä»–"

    text_norm = re.sub(r"\s+", "", text)
    hits = []

    for rule in CATEGORY_RULES:
        for kw in rule["keywords"]:
            if kw in text_norm:
                hits.append(rule["short"])
                break

    if not hits:
        return "æœªåˆ†ç±»/å…¶ä»–"

    return "ã€".join(hits)


# ===== 2. é¢„å¤„ç†ï¼šè§£å†³æ··åˆè¯­è¨€åˆ†è¯ & æ¸…æ´—å…ƒæ•°æ® =====

def smart_tokenize_for_jieba(text: str) -> str:
    """
    ã€ä¿®æ­£æ··åˆè¯­è¨€çš„å…³é”®æ­¥éª¤ã€‘: ç¡®ä¿è¿ç»­çš„è‹±æ–‡/æ•°å­—/ç¬¦å·è¢« Jieba è¯†åˆ«ä¸ºä¸€ä¸ª Tokenã€‚
    """
    tokens = re.findall(r'[\u4e00-\u9fa5]+|[^\u4e00-\u9fa5]+', text)
    return " ".join(tokens).strip()


def clean_meta_fields(line: str) -> str:
    """å»æ‰ç±»ç›®å­—æ®µå’Œçƒ­åº¦æ•°å­—"""
    if not line:
        return ""

    text = line.strip()

    if text in STOP_CATEGORIES:
        return ""
    for cat in STOP_CATEGORIES:
        text = text.replace(cat, "")

    for pat in PLATFORM_NUM_PATTERNS:
        text = pat.sub("", text)

    return text.strip()


def preprocess_docs(raw_text: str):
    """æŠŠè¾“å…¥çš„å¤§æ–‡æœ¬ï¼Œæ‹†æˆâ€œæ–‡æ¡ˆåˆ—è¡¨ docsâ€ï¼Œå¹¶é¢„æ¸…æ´—"""
    docs = []
    for line in raw_text.splitlines():
        line = clean_meta_fields(line)
        if not line:
            continue
        if len(line) < 3:
            continue
        docs.append(line)
    return docs


# ===== 3. PMI è®¡ç®— & çŸ­è¯­æå– (é›†æˆ A çº§æƒé‡) =====

def build_pmi_and_doc_phrases(docs,
                              min_freq: int,
                              min_len: int,
                              max_len: int,
                              weight: int):
    """è®¡ç®— PMIï¼Œå¹¶è¿›è¡ŒçŸ­è¯­æå–"""

    tokenized_docs = []
    for doc in docs:
        preprocessed_doc = smart_tokenize_for_jieba(doc)
        tokens = [t.strip() for t in jieba.lcut(preprocessed_doc) if t.strip()]
        tokenized_docs.append(tokens)

    # ç»Ÿè®¡è¯é¢‘å¹¶åº”ç”¨æƒé‡
    unigram = Counter()
    bigram = Counter()

    for tokens in tokenized_docs:
        unigram.update({t: weight for t in tokens})
        for i in range(len(tokens) - 1):
            bg = (tokens[i], tokens[i + 1])
            bigram.update({bg: weight})

    total_unigrams = sum(unigram.values()) or 1
    total_bigrams = sum(bigram.values()) or 1

    # è®¡ç®—æ‰€æœ‰ bi-gram çš„ PMI
    phrase_pmi = {}
    for (w1, w2), c12 in bigram.items():
        phrase = w1 + w2

        # é¢‘ç‡è¿‡æ»¤ (æ³¨æ„ c12 æ˜¯åŠ æƒè®¡æ•°)
        if c12 < min_freq * weight:
            continue

        if looks_like_noise(phrase):
            continue
        if not (min_len <= len(phrase) <= max_len):
            continue

        c1 = unigram[w1]
        c2 = unigram[w2]
        if c1 <= 0 or c2 <= 0:
            continue

        # PMI è®¡ç®—
        p12 = c12 / total_bigrams
        p1 = c1 / total_unigrams
        p2 = c2 / total_unigrams

        pmi = math.log((p12 / (p1 * p2 + 1e-9)) + 1e-9, 2)

        if pmi > 3.0:
            phrase_pmi[phrase] = pmi

    # æ¯æ¡æ–‡æ¡ˆåªé€‰ 1 ä¸ª PMI æœ€é«˜çš„çŸ­è¯­
    phrase_doc_count = Counter()
    phrase_example = {}

    for idx, tokens in enumerate(tokenized_docs):
        best_phrase = None
        best_pmi = -1e9

        for i in range(len(tokens) - 1):
            phrase = tokens[i] + tokens[i + 1]
            pmi = phrase_pmi.get(phrase, -1)

            if pmi > best_pmi:
                best_pmi = pmi
                best_phrase = phrase

        if best_phrase:
            phrase_doc_count[best_phrase] += 1
            phrase_example.setdefault(best_phrase, docs[idx])

    return phrase_pmi, phrase_doc_count, phrase_example


def build_result_df(phrase_pmi,
                    phrase_doc_count,
                    phrase_example,
                    top_k: int):
    """
    æ„å»ºç»“æœè¡¨ï¼š
    - åœ¨è¿™é‡Œåšâ€œæœ€ç»ˆç»“æœè¿‡æ»¤â€ï¼šå‰”é™¤å«äººå / å›½å®¶åçš„çŸ­è¯­
    """
    rows = []
    for phrase, freq in phrase_doc_count.most_common(top_k):
        example = phrase_example.get(phrase, "")

        # === å…³é”®æ–°å¢ï¼šè¿‡æ»¤äººå / å›½å®¶å ===
        if contains_country_or_name(phrase) or contains_country_or_name(example):
            # ç›´æ¥è·³è¿‡ï¼Œä¸è¿›å…¥æœ€ç»ˆç»“æœ
            continue

        category = classify_example_text(example)
        rows.append({
            "çŸ­è¯­": phrase,
            "æ–‡æ¡ˆé¢‘æ¬¡": freq,
            "PMIå‡å›ºåº¦": round(phrase_pmi.get(phrase, 0.0), 2),
            "å­—ç¬¦é•¿åº¦": len(phrase),
            "ç¤ºä¾‹æ–‡æœ¬": example,
            "ä¸»é¢˜åˆ†ç±»": category
        })
    df = pd.DataFrame(
        rows,
        columns=["çŸ­è¯­", "æ–‡æ¡ˆé¢‘æ¬¡", "PMIå‡å›ºåº¦", "å­—ç¬¦é•¿åº¦", "ç¤ºä¾‹æ–‡æœ¬", "ä¸»é¢˜åˆ†ç±»"]
    )
    return df


# ===== 3.5 é˜¿é‡Œäº‘ç™¾ç‚¼ LLM è°ƒç”¨ï¼šå¯é€‰æ·±åº¦åˆ†æ =====

def llm_analyze_phrase(api_key: str, phrase: str, example_text: str) -> str:
    """
    è°ƒç”¨é˜¿é‡Œäº‘ç™¾ç‚¼çš„é€šä¹‰æ¨¡å‹ï¼Œå¯¹å•ä¸ªçƒ­æ¢—åšæ·±åº¦åˆ†æã€‚
    """
    url = "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"

    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªçŸ­è§†é¢‘&å†…å®¹è¥é”€çš„çƒ­æ¢—åˆ†æä¸“å®¶ï¼Œè¯·å¸®åŠ©æˆ‘ä»è¿è¥è§†è§’è¯»æ‡‚ä¸€ä¸ªæ¢—ã€‚

ã€çŸ­è¯­ã€‘ï¼š
{phrase}

ã€ç¤ºä¾‹æ–‡æ¡ˆã€‘ï¼š
{example_text}

è¯·æŒ‰ä»¥ä¸‹ç»“æ„è¾“å‡ºï¼ˆä¸­æ–‡ï¼‰ï¼š

1. æ¢—åœºæ™¯ç±»å‹ï¼ˆç”¨ä½ è‡ªå·±çš„è¯å‘½åï¼Œä¾‹å¦‚ï¼šå†·ç©ºæ°”çªè¢­ã€æ‰“å·¥äººä¸‹ç­å´©æºƒã€å¦ˆå‘³å®¶åŠ¡ã€å® ç‰©å½“å°å­©ã€è‡ªæˆ‘å¥–åŠ±æ¶ˆè´¹ç­‰ï¼‰
2. æ¢—çš„çœŸå®å«ä¹‰ï¼ˆ1-2 å¥ï¼‰
3. å…¸å‹è§¦å‘åœºæ™¯ï¼ˆä»€ä¹ˆäººã€åœ¨ä»€ä¹ˆæ—¶åˆ»/æƒ…ç»ªä¸‹ä¼šè¯´è¿™å¥è¯ï¼‰
4. å¯¹å®¶ç”µå“ç‰Œçš„é€‚é…å»ºè®®ï¼ˆé€‚åˆå“ªäº›å“ç±»ï¼Ÿå¦‚ï¼šç©ºè°ƒ/çƒ­æ°´å™¨/æ´—è¡£æœº/å†°ç®±/çƒ˜å¹²æœº/å–æš–å™¨ç­‰ï¼Œå¹¶ç»™å‡ºç†ç”±ï¼‰
5. 3 æ¡å¯ç›´æ¥å‚è€ƒçš„åˆ›æ„ç©æ³•æ–‡æ¡ˆï¼ˆé€‚åˆåšæ ‡é¢˜/å£æ’­/è„šæœ¬çš„çŸ­å¥ï¼‰

è¡¥å……è¯´æ˜ï¼š
- ä½ å¯ä»¥åœ¨ç»“å°¾ç®€å•è¯´æ˜ï¼šè¿™ä¸ªæ¢—å¤§è‡´æ¥è¿‘å“ªä¸€ç±»ï¼ˆå†·ç©ºæ°”/å¤©æ°”ã€å®¶é‡Œäºº/å®¶åŠ¡ã€æ‰“å·¥äºº/åŸå¸‚ã€å® ç‰©ã€ç”Ÿæ´»æ–¹å¼/å®¡ç¾ï¼‰ï¼Œæˆ–è€…è¯´æ˜â€œä¸åœ¨è¿™äº›é‡Œâ€ï¼Œçœ‹ä½ çš„åˆ¤æ–­ã€‚
- ä¸è¦è§£é‡Šä½ åœ¨åšä»€ä¹ˆï¼Œç›´æ¥ç»™å‡ºæ¡ç›®ã€‚
"""

    payload = {
        "model": "qwen-plus",  # å…ˆç”¨æ€§ä»·æ¯”é«˜çš„ plus
        "input": {
            "messages": [
                {"role": "user", "content": prompt}
            ]
        }
    }

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }

    resp = requests.post(url, headers=headers, json=payload, timeout=60)
    data = resp.json()

    # å°è¯•ä»æ ‡å‡†å­—æ®µå–å†…å®¹
    try:
        return data["output"]["text"]
    except Exception:
        # å‡ºé”™æ—¶åŸæ ·è¿”å›ï¼Œæ–¹ä¾¿æ’æŸ¥
        return str(data)


# ===== 4. Streamlit é¡µé¢ (UI) =====

def main():
    st.set_page_config(page_title="æœ¬åœ°çƒ­æ¢—æå–å·¥å…· (PMI è¶‹åŠ¿ç‰ˆ + LLM æ·±åº¦åˆ†æ)",
                       layout="wide")

    st.title("ğŸ“Œ æœ¬åœ°çƒ­é—¨è¶‹åŠ¿ / çƒ­æ¢—æå–å·¥å…·ï¼ˆPMI å¢å¼ºç‰ˆ + é€šä¹‰ LLM æ·±åº¦åˆ†æï¼‰")

    with st.sidebar:
        st.header("âš™ï¸ åˆ†æå‚æ•°è®¾ç½®")

        weight = st.slider("æ•°æ®æƒé‡ W (Açº§æ•°æ®æ¨¡æ‹Ÿ)",
                           min_value=1,
                           max_value=10,
                           value=5,
                           help="æé«˜æƒé‡æ¨¡æ‹Ÿé«˜ä»·å€¼/æƒå¨æ•°æ®æºã€‚æƒé‡è¶Šé«˜ï¼Œä½é¢‘ä¼˜è´¨æ¢—è¶Šå®¹æ˜“å…¥é€‰ã€‚")

        min_freq = st.slider("æœ€å°æ–‡æ¡ˆå‡ºç°æ¬¡æ•°",
                             min_value=1,
                             max_value=20,
                             value=2)

        st.markdown(f"**æœ€ä½æœ‰æ•ˆé¢‘æ¬¡ (åŠ æƒå):** **{min_freq * weight}** (å®é™…è®¡ç®—æ¬¡æ•°)")
        st.markdown("---")

        top_k = st.slider("å±•ç¤º TopK æ•°é‡", min_value=10, max_value=200, value=50)
        min_len = st.slider("æœ€å°å­—ç¬¦é•¿åº¦", min_value=2, max_value=4, value=2)
        max_len = st.slider("æœ€å¤§å­—ç¬¦é•¿åº¦", min_value=3, max_value=8, value=5)

    st.markdown("### ğŸ“¥ ç²˜è´´æ–°çš„åŸå§‹æ–‡æ¡ˆæ•°æ®")
    st.info("ğŸ’¡ **æ“ä½œè¯´æ˜:** å°†å¤šæ¡æ–‡æ¡ˆç›´æ¥ç²˜è´´åˆ°ä¸‹æ–¹ï¼Œ**æ¯æ¡æ–‡æ¡ˆç‹¬å ä¸€è¡Œ**ã€‚ç¨‹åºå°†è‡ªåŠ¨è¿‡æ»¤ç±»ç›®ã€æ•°å­—å’Œå™ªéŸ³ï¼Œå¹¶ä¿®æ­£æ··åˆè¯­è¨€åˆ†è¯ã€‚")

    default_text = """
è€å¼è¿‡å†¬æ˜¯ä¸€ç§è¶‹åŠ¿
ä»Šå¤©å†·ç©ºæ°”æœ‰ç‚¹æŠ½è±¡
ä¸‹ç­å›å®¶ç¬¬ä¸€ä»¶äº‹å°±æ˜¯å¼€ç©ºè°ƒ
æ‰“å·¥äººçš„å†¬å¤©æµç¨‹belikeï¼šä¸‹ç­-æ´—ä¸ªçƒ­æ°´æ¾¡-é’»è¢«çª
æˆ‘å¦ˆå«Œå†·è®©æˆ‘åˆ«æ´—å¤´
çŒ«çŒ«å†¬å¤©ä¹Ÿè¦æœ‰è‡ªå·±çš„å°è¢«çª
100å¤©å†¬å­£è¿åŠ¨æ‰“å¡ä»Šå¤©ç®—ç¬¬3å¤©
"""

    raw_text = st.text_area(
        "",
        value=default_text,
        height=300,
        placeholder="æ¯è¡Œä¸€æ¡æ–‡æ¡ˆï¼Œå¯ä»¥ç›´æ¥ä» Excel å¤åˆ¶å¤šåˆ—ç²˜è¿‡æ¥ï¼Œç¨‹åºä¼šè‡ªåŠ¨æ¸…æ´—å’Œåˆ†æã€‚"
    )

    df = None

    if st.button("ğŸš€ ç¬¬ä¸€æ­¥ï¼šæå–çƒ­é—¨è¶‹åŠ¿ / çƒ­æ¢—ï¼ˆæœ¬åœ° PMIï¼Œä¸è€— tokenï¼‰", use_container_width=True):
        if not raw_text.strip():
            st.warning("è¯·å…ˆç²˜è´´ä¸€äº›æ–‡æ¡ˆå†ç‚¹å‡»æŒ‰é’®ã€‚")
            return

        with st.spinner('æ­£åœ¨è¿›è¡Œæ™ºèƒ½åˆ†è¯ä¸ PMI åŠ æƒè®¡ç®—...'):
            docs = preprocess_docs(raw_text)
            if not docs:
                st.warning("æœ‰æ•ˆæ–‡æ¡ˆä¸ºç©ºï¼Œå¯èƒ½éƒ½è¢«å½“æˆç±»ç›®/å™ªéŸ³è¿‡æ»¤æ‰äº†ã€‚")
                return

            phrase_pmi, phrase_doc_count, phrase_example = build_pmi_and_doc_phrases(
                docs,
                min_freq=min_freq,
                min_len=min_len,
                max_len=max_len,
                weight=weight  # ä¼ å…¥æƒé‡
            )

            df = build_result_df(
                phrase_pmi,
                phrase_doc_count,
                phrase_example,
                top_k=top_k,
            )

        st.success("æå–å®Œæˆï¼šå·²åº”ç”¨åŠ æƒåˆ†æã€ä¿®æ­£æ··åˆè¯­è¨€åˆ†è¯ï¼Œå¹¶å®Œæˆè§„åˆ™åœºæ™¯åˆ†ç±»ã€‚"
                   "ï¼ˆå·²è‡ªåŠ¨å‰”é™¤å«äººå / å›½å®¶åçš„çŸ­è¯­ï¼‰")

        st.subheader("âœ… å€™é€‰çƒ­é—¨è¶‹åŠ¿ / çƒ­æ¢—ç»“æœï¼ˆæœ¬åœ°åˆ†æï¼‰")
        st.dataframe(df, use_container_width=True)

        # ç®€å•çœ‹ä¸€ä¸‹ä¸»é¢˜åˆ†å¸ƒ
        if not df.empty:
            st.markdown("#### ğŸ“Š è§„åˆ™åˆ†ç±»ä¸‹çš„ä¸»é¢˜åˆ†å¸ƒï¼ˆå‚è€ƒç”¨ï¼‰")
            category_counts = df["ä¸»é¢˜åˆ†ç±»"].value_counts().reset_index()
            category_counts.columns = ["ä¸»é¢˜åˆ†ç±»", "çŸ­è¯­æ•°é‡"]
            st.table(category_counts)

        # æŠŠ df å­˜åˆ° session_stateï¼Œæ–¹ä¾¿åé¢ LLM ç”¨
        st.session_state["last_df"] = df

    # ===== ç¬¬äºŒæ­¥ï¼šå¯é€‰ LLM æ·±åº¦åˆ†æ =====
    st.markdown("---")
    st.subheader("âœ¨ ç¬¬äºŒæ­¥ï¼ˆå¯é€‰ï¼‰ï¼šä½¿ç”¨é€šä¹‰å¤§æ¨¡å‹è¿›è¡Œæ·±åº¦è¯­ä¹‰åˆ†æï¼ˆæŒ‰éœ€è°ƒç”¨ï¼ŒèŠ‚çº¦ tokenï¼‰")

    enable_llm = st.checkbox("å¼€å¯é€šä¹‰å¤§æ¨¡å‹åˆ†æï¼ˆä»…å¯¹å‰ N ä¸ªçŸ­è¯­è°ƒç”¨ï¼‰")

    if enable_llm:
        api_key = st.text_input(
            "è¯·è¾“å…¥ä½ çš„é˜¿é‡Œäº‘ç™¾ç‚¼ API-Keyï¼ˆä¸ä¼šè¢«ä¿å­˜ï¼‰ï¼š",
            type="password",
            help="åœ¨ç™¾ç‚¼æ§åˆ¶å°çš„ã€Œå¯†é’¥ç®¡ç†ã€é‡Œå¯ä»¥æ‰¾åˆ°ä»¥ sk- å¼€å¤´çš„ API Keyã€‚"
        )

        last_df = st.session_state.get("last_df", None)
        if last_df is None or last_df.empty:
            st.info("è¯·å…ˆå®Œæˆä¸Šé¢çš„ã€ç¬¬ä¸€æ­¥ï¼šæœ¬åœ° PMI æå–ã€ï¼Œå†è¿›è¡Œå¤§æ¨¡å‹åˆ†æã€‚")
            return

        max_rows = st.slider(
            "é€‰æ‹©è¦ç”¨ LLM æ·±åº¦åˆ†æçš„æ¢—æ•°é‡ï¼ˆæŒ‰å½“å‰æ’åºçš„å‰ N æ¡ï¼‰ï¼š",
            min_value=5,
            max_value=min(50, len(last_df)),
            value=min(20, len(last_df))
        )

        if st.button("ğŸš€ å¼€å§‹å¤§æ¨¡å‹åˆ†æï¼ˆæŒ‰éœ€æ¶ˆè€— tokensï¼‰", use_container_width=True):
            if not api_key:
                st.warning("è¯·å…ˆè¾“å…¥ API-Keyã€‚")
                return

            target_df = last_df.head(max_rows).copy()
            llm_results = []

            for idx, row in target_df.iterrows():
                phrase = row["çŸ­è¯­"]
                example = row["ç¤ºä¾‹æ–‡æœ¬"]

                with st.spinner(f"æ­£åœ¨åˆ†æï¼š{phrase} ..."):
                    analysis = llm_analyze_phrase(api_key, phrase, example)

                llm_results.append(analysis)

            target_df["LLMæ·±åº¦æ´å¯Ÿ"] = llm_results

            st.success("å¤§æ¨¡å‹åˆ†æå®Œæˆ âœ…ï¼ˆä»…å¯¹å‰ N æ¡è¿›è¡Œäº†å¤„ç†ï¼‰")

            st.subheader("ğŸ“Š å«å¤§æ¨¡å‹æ´å¯Ÿçš„ç»“æœï¼ˆTop Nï¼‰")
            st.dataframe(
                target_df[
                    ["çŸ­è¯­", "æ–‡æ¡ˆé¢‘æ¬¡", "PMIå‡å›ºåº¦", "å­—ç¬¦é•¿åº¦", "ç¤ºä¾‹æ–‡æœ¬", "ä¸»é¢˜åˆ†ç±»", "LLMæ·±åº¦æ´å¯Ÿ"]
                ],
                use_container_width=True
            )


if __name__ == "__main__":
    main()
